{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /usr/local/lib/python3.7/site-packages (2.0.0a0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.16.1)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.19.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.32.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
      "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.1.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.6.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import scipy\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For the network\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import ipywidgets as iw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "!pip install tensorflow==2.0.0-alpha0\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers, models, optimizers, metrics \n",
    "\n",
    "\n",
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Import various componenets for model building\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import LSTM, Input, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from tensorflow.python.keras import backend as k\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_18_m = pd.read_csv('traffic_18_m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_18_m['speed_index'] = 0\n",
    "\n",
    "# Minus\n",
    "traffic_18_m['speed_index'].loc[(traffic_18_m['SPEED']<=(traffic_18_m['speed_limit']-10))&\n",
    "                                  (traffic_18_m['SPEED']>(traffic_18_m['speed_limit']-20))] = -10\n",
    "traffic_18_m['speed_index'].loc[(traffic_18_m['SPEED']<=(traffic_18_m['speed_limit']-20))] = -20\n",
    "\n",
    "\n",
    "# Minus\n",
    "traffic_18_m['speed_index'].loc[(traffic_18_m['SPEED']>=(traffic_18_m['speed_limit']+10))&\n",
    "                                  (traffic_18_m['SPEED']<(traffic_18_m['speed_limit']+20))] = 10\n",
    "traffic_18_m['speed_index'].loc[(traffic_18_m['SPEED']>=(traffic_18_m['speed_limit']+20))] = 20\n",
    "\n",
    "traffic_18_m['speed_index'].loc[traffic_18_m['SPEED']==0] = -100\n",
    "\n",
    "\n",
    "\n",
    "#traffic_18_m['speed_index'] = traffic_18_m['speed_index'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10     500000\n",
       "-20     500000\n",
       "-100    500000\n",
       " 20     500000\n",
       " 10     500000\n",
       " 0      500000\n",
       "Name: speed_index, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_s = traffic_18_m.sample(frac=1, random_state=1)\n",
    "\n",
    "traffic_cnn = pd.DataFrame(columns=traffic_s.columns)\n",
    "\n",
    "for i in traffic_s.speed_index.unique():\n",
    "    label = traffic_s.loc[traffic_s['speed_index']==i][:500000]\n",
    "    traffic_cnn = pd.concat([traffic_cnn, label])\n",
    "    \n",
    "traffic_cnn.speed_index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                 object\n",
       "index                      object\n",
       "ID                         object\n",
       "SPEED                     float64\n",
       "TRAVEL_TIME                object\n",
       "STATUS                     object\n",
       "DATA_AS_OF                 object\n",
       "LINK_ID                    object\n",
       "LINK_POINTS                object\n",
       "ENCODED_POLY_LINE          object\n",
       "ENCODED_POLY_LINE_LVLS     object\n",
       "OWNER                      object\n",
       "TRANSCOM_ID                object\n",
       "BOROUGH                    object\n",
       "LINK_NAME                  object\n",
       "year                       object\n",
       "month                      object\n",
       "dayofweek                  object\n",
       "RecordedAtTime             object\n",
       "hour                       object\n",
       "minute                     object\n",
       "poly_num                   object\n",
       "LINK_START                 object\n",
       "LINK_END                   object\n",
       "LINK_MIDDLE                object\n",
       "speed_id                   object\n",
       "speed_limit                object\n",
       "lion_id                    object\n",
       "Street                     object\n",
       "FeatureTyp                 object\n",
       "SegmentTyp                 object\n",
       "RB_Layer                   object\n",
       "NonPed                     object\n",
       "TrafDir                    object\n",
       "SeqNum                     object\n",
       "StreetCode                 object\n",
       "StreetWidt                float64\n",
       "BikeLane                   object\n",
       "Snow_Prior                 object\n",
       "Number_Tra                 object\n",
       "Number_Tot                 object\n",
       "Join_ID                    object\n",
       "weekend                    object\n",
       "morn_rush_hr               object\n",
       "eve_rush_hr                object\n",
       "morning_rush_avg_spd      float64\n",
       "evening_rush_avg_spd      float64\n",
       "wknd_avg_spd              float64\n",
       "overall_avg_spd           float64\n",
       "overall_std_speed         float64\n",
       "speed_index                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_cnn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_cnn = traffic_cnn.drop(columns=['Unnamed: 0','index','LINK_POINTS','ENCODED_POLY_LINE','ENCODED_POLY_LINE_LVLS','TRANSCOM_ID','Join_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonumeric = ['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour','minute','poly_num','BikeLane', \n",
    "             'weekend','morn_rush_hr', 'eve_rush_hr','Number_Tot','Number_Tra','SeqNum','StreetCode','lion_id',\n",
    "             'speed_id','speed_limit']\n",
    "tocategory = ['Snow_Prior','NonPed','RB_Layer','SegmentTyp','FeatureTyp','Street','BOROUGH']\n",
    "\n",
    "# turn columns into numberic\n",
    "for i in tonumeric:\n",
    "    traffic_cnn[i] = pd.to_numeric(traffic_cnn[i])\n",
    "\n",
    "# to a category then immediately into a coded column\n",
    "for i in tocategory:\n",
    "    traffic_cnn[i] = traffic_cnn[i].astype('category')\n",
    "    traffic_cnn[i+'_codes'] = traffic_cnn[i].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_cnn = traffic_cnn.drop(columns='NonPed')\n",
    "traffic_cnn = traffic_cnn.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_cnn = traffic_cnn.drop(['DATA_AS_OF','OWNER', 'BOROUGH','LINK_NAME','RecordedAtTime',\n",
    "                                'LINK_START','LINK_END','LINK_MIDDLE','Street', 'FeatureTyp', 'SegmentTyp', \n",
    "                                'RB_Layer', 'TrafDir','Snow_Prior', 'TRAVEL_TIME','SPEED'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traffic_cnn['speed_index'] = pd.to_numeric(traffic_cnn['speed_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                        int64\n",
       "STATUS                    int64\n",
       "LINK_ID                   int64\n",
       "year                      int64\n",
       "month                     int64\n",
       "dayofweek                 int64\n",
       "hour                      int64\n",
       "minute                    int64\n",
       "poly_num                  int64\n",
       "speed_id                  int64\n",
       "speed_limit               int64\n",
       "lion_id                   int64\n",
       "SeqNum                    int64\n",
       "StreetCode                int64\n",
       "StreetWidt              float64\n",
       "BikeLane                  int64\n",
       "Number_Tra                int64\n",
       "Number_Tot                int64\n",
       "weekend                   int64\n",
       "morn_rush_hr              int64\n",
       "eve_rush_hr               int64\n",
       "morning_rush_avg_spd    float64\n",
       "evening_rush_avg_spd    float64\n",
       "wknd_avg_spd            float64\n",
       "overall_avg_spd         float64\n",
       "overall_std_speed       float64\n",
       "speed_index              object\n",
       "Snow_Prior_codes           int8\n",
       "NonPed_codes               int8\n",
       "RB_Layer_codes             int8\n",
       "SegmentTyp_codes           int8\n",
       "FeatureTyp_codes           int8\n",
       "Street_codes               int8\n",
       "BOROUGH_codes              int8\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_cnn.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('speed_index')                     \n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 34)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to split a training and test sample\n",
    "traffic_train, traffic_test = train_test_split(traffic_cnn, test_size=0.5, random_state=0)\n",
    "traffic_test, traffic_val = train_test_split(traffic_test, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500000, 34)\n",
      "(750000, 34)\n",
      "(750000, 34)\n"
     ]
    }
   ],
   "source": [
    "print(traffic_train.shape)\n",
    "print(traffic_test.shape)\n",
    "print(traffic_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = df_to_dataset(traffic_train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(traffic_val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(traffic_test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour', 'minute', 'poly_num', 'speed_id', 'speed_limit', 'lion_id', 'SeqNum', 'StreetCode', 'StreetWidt', 'BikeLane', 'Number_Tra', 'Number_Tot', 'weekend', 'morn_rush_hr', 'eve_rush_hr', 'morning_rush_avg_spd', 'evening_rush_avg_spd', 'wknd_avg_spd', 'overall_avg_spd', 'overall_std_speed', 'Snow_Prior_codes', 'NonPed_codes', 'RB_Layer_codes', 'SegmentTyp_codes', 'FeatureTyp_codes', 'Street_codes', 'BOROUGH_codes']\n",
      "A batch of Street Widths: tf.Tensor(\n",
      "[34. 32. 26. 36. 60. 34. 30. 24. 30. 80. 32. 32. 36. 25. 32. 34. 40. 36.\n",
      " 26. 36. 18. 20. 32. 40. 32. 20. 20. 30. 25. 36. 32. 30. 32. 32. 20. 24.\n",
      " 42. 60. 42. 20. 42. 36. 60. 36. 18. 32. 30. 30. 20. 40. 38. 20. 36. 34.\n",
      " 20. 33. 33. 33. 42. 25. 33. 32. 26. 20.], shape=(64,), dtype=float32)\n",
      "A batch of targets: tf.Tensor(\n",
      "[  10    0 -100   20   10   10    0   10  -20    0   10  -10  -10    0\n",
      "  -10   20   20  -10  -10  -20   20 -100  -20  -20   10 -100   10  -20\n",
      "   10    0   10  -20    0    0  -10   10  -10    0    0  -10  -20  -20\n",
      " -100    0  -10    0  -20 -100  -10   20  -20    0   20  -10    0   10\n",
      "  -20    0 -100    0    0  -20 -100  -10], shape=(64,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print('Every feature:', list(feature_batch.keys()))\n",
    "    print('A batch of Street Widths:', feature_batch['StreetWidt'])\n",
    "    print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def demo(feature_column):\n",
    "    feature_layer = layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour',\n",
       "       'minute', 'poly_num', 'speed_id', 'speed_limit', 'lion_id', 'SeqNum',\n",
       "       'StreetCode', 'StreetWidt', 'BikeLane', 'Number_Tra', 'Number_Tot',\n",
       "       'weekend', 'morn_rush_hr', 'eve_rush_hr', 'morning_rush_avg_spd',\n",
       "       'evening_rush_avg_spd', 'wknd_avg_spd', 'overall_avg_spd',\n",
       "       'overall_std_speed', 'speed_index', 'Snow_Prior_codes', 'NonPed_codes',\n",
       "       'RB_Layer_codes', 'SegmentTyp_codes', 'FeatureTyp_codes',\n",
       "       'Street_codes', 'BOROUGH_codes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_cnn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0528 23:29:08.985878 4616742336 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2758: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.]\n",
      " [ 3.]\n",
      " [11.]\n",
      " [ 3.]\n",
      " [ 7.]\n",
      " [14.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [14.]\n",
      " [ 7.]\n",
      " [ 6.]\n",
      " [16.]\n",
      " [20.]\n",
      " [ 2.]\n",
      " [ 8.]\n",
      " [15.]\n",
      " [ 1.]\n",
      " [ 8.]\n",
      " [16.]\n",
      " [16.]\n",
      " [ 9.]\n",
      " [17.]\n",
      " [23.]\n",
      " [22.]\n",
      " [14.]\n",
      " [23.]\n",
      " [ 5.]\n",
      " [11.]\n",
      " [14.]\n",
      " [14.]\n",
      " [19.]\n",
      " [16.]\n",
      " [22.]\n",
      " [ 7.]\n",
      " [16.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [13.]\n",
      " [ 5.]\n",
      " [14.]\n",
      " [ 0.]\n",
      " [19.]\n",
      " [14.]\n",
      " [13.]\n",
      " [17.]\n",
      " [13.]\n",
      " [13.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [22.]\n",
      " [19.]\n",
      " [10.]\n",
      " [ 0.]\n",
      " [19.]\n",
      " [17.]\n",
      " [ 5.]\n",
      " [23.]\n",
      " [ 5.]\n",
      " [ 9.]\n",
      " [17.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [16.]\n",
      " [ 7.]]\n"
     ]
    }
   ],
   "source": [
    "hour = feature_column.numeric_column('hour')\n",
    "demo(hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(feature_batch.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour',\n",
       "       'minute', 'poly_num', 'speed_id', 'speed_limit', 'lion_id', 'SeqNum',\n",
       "       'StreetCode', 'StreetWidt', 'BikeLane', 'Number_Tra', 'Number_Tot',\n",
       "       'weekend', 'morn_rush_hr', 'eve_rush_hr', 'morning_rush_avg_spd',\n",
       "       'evening_rush_avg_spd', 'wknd_avg_spd', 'overall_avg_spd',\n",
       "       'overall_std_speed', 'speed_index', 'Snow_Prior_codes', 'NonPed_codes',\n",
       "       'RB_Layer_codes', 'SegmentTyp_codes', 'FeatureTyp_codes',\n",
       "       'Street_codes', 'BOROUGH_codes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_cnn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 40, 25, 35, 45,  0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_cnn['speed_limit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour',\n",
    "               'minute', 'poly_num', 'speed_id', 'speed_limit', 'lion_id', 'SeqNum',\n",
    "               'StreetCode', 'StreetWidt', 'BikeLane', 'Number_Tra', 'Number_Tot',\n",
    "               'weekend', 'morn_rush_hr', 'eve_rush_hr', 'morning_rush_avg_spd',\n",
    "               'evening_rush_avg_spd', 'wknd_avg_spd', 'overall_avg_spd',\n",
    "               'overall_std_speed', 'Snow_Prior_codes', 'NonPed_codes',\n",
    "               'RB_Layer_codes', 'SegmentTyp_codes', 'FeatureTyp_codes',\n",
    "               'Street_codes', 'BOROUGH_codes']:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# indicator cols\n",
    "limit = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'speed_limit', traffic_cnn['speed_limit'].unique())\n",
    "limit_one_hot = feature_column.indicator_column(limit)\n",
    "feature_columns.append(limit_one_hot)\n",
    "    \n",
    "# embedding cols\n",
    "limit_embedding = feature_column.embedding_column(limit, dimension=8)\n",
    "feature_columns.append(limit_embedding)\n",
    "\n",
    "\n",
    "# crossed cols\n",
    "hour_status = feature_column.crossed_column(['hour', 'STATUS'], hash_bucket_size=1000)\n",
    "hour_status = feature_column.indicator_column(hour_status)\n",
    "feature_columns.append(hour_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericColumn(key='hour', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "46875/46875 [==============================] - 3766s 80ms/step - loss: -244.3378 - accuracy: 0.1509 - val_loss: -257.3061 - val_accuracy: 0.1658\n",
      "Epoch 2/25\n",
      "46875/46875 [==============================] - 4302s 92ms/step - loss: -256.5647 - accuracy: 0.1670 - val_loss: -257.3061 - val_accuracy: 0.1658\n",
      "Epoch 3/25\n",
      "46875/46875 [==============================] - 5961s 127ms/step - loss: -256.5647 - accuracy: 0.1670 - val_loss: -257.3061 - val_accuracy: 0.1658\n",
      "Epoch 4/25\n",
      "46875/46875 [==============================] - 7928s 169ms/step - loss: -256.5549 - accuracy: 0.1670 - val_loss: -257.3061 - val_accuracy: 0.1658\n",
      "Epoch 5/25\n",
      "12792/46875 [=======>......................] - ETA: 1:49:32 - loss: -256.7819 - accuracy: 0.1670"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-205044338005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m model.fit(train_ds,\n\u001b[1;32m     17\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           epochs=25)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;31m# Case 3: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    353\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   1941\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    feature_layer,\n",
    "    layers.Dense(128, activation='relu', input_dim=34),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_18_m = pd.read_csv('traffic_18_m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_18_m = traffic_18_m.drop(['Unnamed: 0', 'index'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_18_m['DATA_AS_OF'] = pd.DatetimeIndex(traffic_18_m['DATA_AS_OF'])\n",
    "\n",
    "traffic_sample = traffic_18_m.loc[(traffic_18_m['DATA_AS_OF']>'2018-5-13')&(traffic_18_m['DATA_AS_OF']<'2018-5-20')]\n",
    "\n",
    "traffic_sample = traffic_sample.drop(columns=['LINK_POINTS','ENCODED_POLY_LINE','ENCODED_POLY_LINE_LVLS','TRANSCOM_ID','Join_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250191, 44)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonumeric = ['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour','minute','poly_num','BikeLane', \n",
    "             'weekend','morn_rush_hr', 'eve_rush_hr','Number_Tot','Number_Tra','SeqNum','StreetCode','lion_id',\n",
    "             'speed_id','speed_limit']\n",
    "tocategory = ['Snow_Prior','NonPed','RB_Layer','SegmentTyp','FeatureTyp','Street','BOROUGH']\n",
    "\n",
    "# turn columns into numberic\n",
    "for i in tonumeric:\n",
    "    traffic_sample[i] = pd.to_numeric(traffic_sample[i])\n",
    "\n",
    "# to a category then immediately into a coded column\n",
    "for i in tocategory:\n",
    "    traffic_sample[i] = traffic_sample[i].astype('category')\n",
    "    traffic_sample[i+'_codes'] = traffic_sample[i].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'SPEED', 'TRAVEL_TIME', 'STATUS', 'DATA_AS_OF', 'LINK_ID',\n",
       "       'OWNER', 'BOROUGH', 'LINK_NAME', 'year', 'month', 'dayofweek',\n",
       "       'RecordedAtTime', 'hour', 'minute', 'poly_num', 'LINK_START',\n",
       "       'LINK_END', 'LINK_MIDDLE', 'speed_id', 'speed_limit', 'lion_id',\n",
       "       'Street', 'FeatureTyp', 'SegmentTyp', 'RB_Layer', 'NonPed', 'TrafDir',\n",
       "       'SeqNum', 'StreetCode', 'StreetWidt', 'BikeLane', 'Snow_Prior',\n",
       "       'Number_Tra', 'Number_Tot', 'weekend', 'morn_rush_hr', 'eve_rush_hr',\n",
       "       'morning_rush_avg_spd', 'evening_rush_avg_spd', 'wknd_avg_spd',\n",
       "       'overall_avg_spd', 'overall_std_speed', 'speed_index',\n",
       "       'Snow_Prior_codes', 'NonPed_codes', 'RB_Layer_codes',\n",
       "       'SegmentTyp_codes', 'FeatureTyp_codes', 'Street_codes',\n",
       "       'BOROUGH_codes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_sample = traffic_sample.drop(['speed_index','OWNER', 'BOROUGH','LINK_NAME','RecordedAtTime',\n",
    "                                      'LINK_START','LINK_END','LINK_MIDDLE','Street', 'FeatureTyp', 'SegmentTyp', \n",
    "                                      'RB_Layer', 'TrafDir','Snow_Prior', 'TRAVEL_TIME','NonPed'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = traffic_sample.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167627 82564\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 167623 samples, validate on 82560 samples\n",
      "Epoch 1/25\n",
      "167623/167623 [==============================] - 17s 101us/sample - loss: 0.0044 - accuracy: 0.0230 - val_loss: 1.3061e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/25\n",
      "167623/167623 [==============================] - 17s 101us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 1.3227e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/25\n",
      "167623/167623 [==============================] - 17s 101us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 9.1078e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/25\n",
      "167623/167623 [==============================] - 16s 96us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 6.8509e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/25\n",
      "167623/167623 [==============================] - 16s 97us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 9.7752e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/25\n",
      "167623/167623 [==============================] - 17s 100us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 8.1630e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/25\n",
      "167623/167623 [==============================] - 17s 102us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 6.1894e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/25\n",
      "167623/167623 [==============================] - 19s 115us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 7.7558e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/25\n",
      "167623/167623 [==============================] - 21s 124us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 1.3394e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/25\n",
      "167623/167623 [==============================] - 22s 130us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 8.3949e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/25\n",
      "167623/167623 [==============================] - 18s 107us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 1.4309e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/25\n",
      "167623/167623 [==============================] - 17s 103us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 1.6654e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/25\n",
      "167623/167623 [==============================] - 17s 104us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 6.5204e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/25\n",
      "167623/167623 [==============================] - 18s 107us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 6.5759e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/25\n",
      "167623/167623 [==============================] - 17s 101us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 6.2025e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/25\n",
      "167623/167623 [==============================] - 17s 102us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 1.0469e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/25\n",
      "167623/167623 [==============================] - 17s 100us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 5.7505e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/25\n",
      "167623/167623 [==============================] - 17s 101us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 9.8153e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/25\n",
      "167623/167623 [==============================] - 17s 101us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 9.5710e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/25\n",
      "167623/167623 [==============================] - 17s 100us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 5.7399e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/25\n",
      "167623/167623 [==============================] - 17s 100us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 2.9291e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/25\n",
      "167623/167623 [==============================] - 18s 107us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 9.0320e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/25\n",
      "167623/167623 [==============================] - 18s 106us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 5.6203e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/25\n",
      "167623/167623 [==============================] - 19s 114us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 1.0553e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/25\n",
      "167623/167623 [==============================] - 18s 110us/sample - loss: 0.0013 - accuracy: 0.0231 - val_loss: 6.6473e-05 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15669d6a0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1, look_back)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(trainX, trainY, epochs=25, batch_size=20, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_18_m['DATA_AS_OF'] = pd.DatetimeIndex(traffic_18_m['DATA_AS_OF'])\n",
    "\n",
    "traffic_sample = traffic_18_m.loc[(traffic_18_m['DATA_AS_OF']>'2018-5-13')&(traffic_18_m['DATA_AS_OF']<'2018-5-20')]\n",
    "\n",
    "traffic_sample = traffic_sample.drop(columns=['LINK_POINTS','ENCODED_POLY_LINE','ENCODED_POLY_LINE_LVLS','TRANSCOM_ID','Join_ID'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250191, 46)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonumeric = ['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour','minute','poly_num','BikeLane', \n",
    "             'weekend','morn_rush_hr', 'eve_rush_hr','Number_Tot','Number_Tra','SeqNum','StreetCode','lion_id',\n",
    "             'speed_id','speed_limit']\n",
    "tocategory = ['Snow_Prior','NonPed','RB_Layer','SegmentTyp','FeatureTyp','Street','BOROUGH']\n",
    "\n",
    "# turn columns into numberic\n",
    "for i in tonumeric:\n",
    "    traffic_sample[i] = pd.to_numeric(traffic_sample[i])\n",
    "\n",
    "# to a category then immediately into a coded column\n",
    "for i in tocategory:\n",
    "    traffic_sample[i] = traffic_sample[i].astype('category')\n",
    "    traffic_sample[i+'_codes'] = traffic_sample[i].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'index', 'ID', 'SPEED', 'TRAVEL_TIME', 'STATUS',\n",
       "       'DATA_AS_OF', 'LINK_ID', 'OWNER', 'BOROUGH', 'LINK_NAME', 'year',\n",
       "       'month', 'dayofweek', 'RecordedAtTime', 'hour', 'minute', 'poly_num',\n",
       "       'LINK_START', 'LINK_END', 'LINK_MIDDLE', 'speed_id', 'speed_limit',\n",
       "       'lion_id', 'Street', 'FeatureTyp', 'SegmentTyp', 'RB_Layer', 'NonPed',\n",
       "       'TrafDir', 'SeqNum', 'StreetCode', 'StreetWidt', 'BikeLane',\n",
       "       'Snow_Prior', 'Number_Tra', 'Number_Tot', 'weekend', 'morn_rush_hr',\n",
       "       'eve_rush_hr', 'morning_rush_avg_spd', 'evening_rush_avg_spd',\n",
       "       'wknd_avg_spd', 'overall_avg_spd', 'overall_std_speed', 'speed_index',\n",
       "       'Snow_Prior_codes', 'NonPed_codes', 'RB_Layer_codes',\n",
       "       'SegmentTyp_codes', 'FeatureTyp_codes', 'Street_codes',\n",
       "       'BOROUGH_codes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_sample = traffic_sample.drop(['Unnamed: 0', 'index','speed_index','OWNER', 'BOROUGH','LINK_NAME', 'RecordedAtTime',\n",
    "                                      'LINK_START','LINK_END','LINK_MIDDLE','Street', 'FeatureTyp', 'SegmentTyp', \n",
    "                                      'RB_Layer', 'TrafDir','Snow_Prior', 'TRAVEL_TIME','NonPed'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_sample.index = traffic_sample['DATA_AS_OF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_sample = traffic_sample.drop(['DATA_AS_OF'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SPEED</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>LINK_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>poly_num</th>\n",
       "      <th>...</th>\n",
       "      <th>wknd_avg_spd</th>\n",
       "      <th>overall_avg_spd</th>\n",
       "      <th>overall_std_speed</th>\n",
       "      <th>Snow_Prior_codes</th>\n",
       "      <th>NonPed_codes</th>\n",
       "      <th>RB_Layer_codes</th>\n",
       "      <th>SegmentTyp_codes</th>\n",
       "      <th>FeatureTyp_codes</th>\n",
       "      <th>Street_codes</th>\n",
       "      <th>BOROUGH_codes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATA_AS_OF</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-19 23:58:09</th>\n",
       "      <td>423</td>\n",
       "      <td>47.84</td>\n",
       "      <td>0</td>\n",
       "      <td>4616299</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>49.801891</td>\n",
       "      <td>46.591828</td>\n",
       "      <td>11.256594</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-19 23:53:09</th>\n",
       "      <td>423</td>\n",
       "      <td>47.22</td>\n",
       "      <td>0</td>\n",
       "      <td>4616299</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>53</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>49.801891</td>\n",
       "      <td>46.591828</td>\n",
       "      <td>11.256594</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-19 23:48:09</th>\n",
       "      <td>423</td>\n",
       "      <td>43.49</td>\n",
       "      <td>0</td>\n",
       "      <td>4616299</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>48</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>49.801891</td>\n",
       "      <td>46.591828</td>\n",
       "      <td>11.256594</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-19 23:43:09</th>\n",
       "      <td>423</td>\n",
       "      <td>37.90</td>\n",
       "      <td>0</td>\n",
       "      <td>4616299</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>49.801891</td>\n",
       "      <td>46.591828</td>\n",
       "      <td>11.256594</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-19 23:38:09</th>\n",
       "      <td>423</td>\n",
       "      <td>37.28</td>\n",
       "      <td>0</td>\n",
       "      <td>4616299</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>49.801891</td>\n",
       "      <td>46.591828</td>\n",
       "      <td>11.256594</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID  SPEED  STATUS  LINK_ID  year  month  dayofweek  \\\n",
       "DATA_AS_OF                                                                 \n",
       "2018-05-19 23:58:09  423  47.84       0  4616299  2018      5          5   \n",
       "2018-05-19 23:53:09  423  47.22       0  4616299  2018      5          5   \n",
       "2018-05-19 23:48:09  423  43.49       0  4616299  2018      5          5   \n",
       "2018-05-19 23:43:09  423  37.90       0  4616299  2018      5          5   \n",
       "2018-05-19 23:38:09  423  37.28       0  4616299  2018      5          5   \n",
       "\n",
       "                     hour  minute  poly_num  ...  wknd_avg_spd  \\\n",
       "DATA_AS_OF                                   ...                 \n",
       "2018-05-19 23:58:09    23      58        44  ...     49.801891   \n",
       "2018-05-19 23:53:09    23      53        44  ...     49.801891   \n",
       "2018-05-19 23:48:09    23      48        44  ...     49.801891   \n",
       "2018-05-19 23:43:09    23      43        44  ...     49.801891   \n",
       "2018-05-19 23:38:09    23      38        44  ...     49.801891   \n",
       "\n",
       "                     overall_avg_spd  overall_std_speed  Snow_Prior_codes  \\\n",
       "DATA_AS_OF                                                                  \n",
       "2018-05-19 23:58:09        46.591828          11.256594                 0   \n",
       "2018-05-19 23:53:09        46.591828          11.256594                 0   \n",
       "2018-05-19 23:48:09        46.591828          11.256594                 0   \n",
       "2018-05-19 23:43:09        46.591828          11.256594                 0   \n",
       "2018-05-19 23:38:09        46.591828          11.256594                 0   \n",
       "\n",
       "                     NonPed_codes  RB_Layer_codes  SegmentTyp_codes  \\\n",
       "DATA_AS_OF                                                            \n",
       "2018-05-19 23:58:09             1               1                 1   \n",
       "2018-05-19 23:53:09             1               1                 1   \n",
       "2018-05-19 23:48:09             1               1                 1   \n",
       "2018-05-19 23:43:09             1               1                 1   \n",
       "2018-05-19 23:38:09             1               1                 1   \n",
       "\n",
       "                     FeatureTyp_codes  Street_codes  BOROUGH_codes  \n",
       "DATA_AS_OF                                                          \n",
       "2018-05-19 23:58:09                 0            26              3  \n",
       "2018-05-19 23:53:09                 0            26              3  \n",
       "2018-05-19 23:48:09                 0            26              3  \n",
       "2018-05-19 23:43:09                 0            26              3  \n",
       "2018-05-19 23:38:09                 0            26              3  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'SPEED', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek',\n",
       "       'hour', 'minute', 'poly_num', 'speed_id', 'speed_limit', 'lion_id',\n",
       "       'SeqNum', 'StreetCode', 'StreetWidt', 'BikeLane', 'Number_Tra',\n",
       "       'Number_Tot', 'weekend', 'morn_rush_hr', 'eve_rush_hr',\n",
       "       'morning_rush_avg_spd', 'evening_rush_avg_spd', 'wknd_avg_spd',\n",
       "       'overall_avg_spd', 'overall_std_speed', 'Snow_Prior_codes',\n",
       "       'NonPed_codes', 'RB_Layer_codes', 'SegmentTyp_codes',\n",
       "       'FeatureTyp_codes', 'Street_codes', 'BOROUGH_codes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250190, 68)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# load dataset\n",
    "dataset = traffic_sample\n",
    "values = dataset.values\n",
    "# integer encode direction\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "values[:,1] = encoder.fit_transform(values[:,1])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "#reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "reframed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = traffic_sample.reset_index()\n",
    "reframed['speed'] = labels['SPEED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var4(t-1)</th>\n",
       "      <th>var5(t-1)</th>\n",
       "      <th>var6(t-1)</th>\n",
       "      <th>var7(t-1)</th>\n",
       "      <th>var8(t-1)</th>\n",
       "      <th>var9(t-1)</th>\n",
       "      <th>var10(t-1)</th>\n",
       "      <th>...</th>\n",
       "      <th>var26(t)</th>\n",
       "      <th>var27(t)</th>\n",
       "      <th>var28(t)</th>\n",
       "      <th>var29(t)</th>\n",
       "      <th>var30(t)</th>\n",
       "      <th>var31(t)</th>\n",
       "      <th>var32(t)</th>\n",
       "      <th>var33(t)</th>\n",
       "      <th>var34(t)</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.933628</td>\n",
       "      <td>0.631148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790037</td>\n",
       "      <td>0.367649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>47.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.933628</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790037</td>\n",
       "      <td>0.367649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>43.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.933628</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790037</td>\n",
       "      <td>0.367649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>37.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.933628</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790037</td>\n",
       "      <td>0.367649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>37.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.933628</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790037</td>\n",
       "      <td>0.367649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>39.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
       "1   0.933628   0.631148        1.0    0.66061        0.0        0.0   \n",
       "2   0.933628   0.622951        1.0    0.66061        0.0        0.0   \n",
       "3   0.933628   0.573770        1.0    0.66061        0.0        0.0   \n",
       "4   0.933628   0.500000        1.0    0.66061        0.0        0.0   \n",
       "5   0.933628   0.491803        1.0    0.66061        0.0        0.0   \n",
       "\n",
       "   var7(t-1)  var8(t-1)  var9(t-1)  var10(t-1)  ...  var26(t)  var27(t)  \\\n",
       "1   0.833333        1.0   0.982759    0.677419  ...  0.790037  0.367649   \n",
       "2   0.833333        1.0   0.896552    0.677419  ...  0.790037  0.367649   \n",
       "3   0.833333        1.0   0.810345    0.677419  ...  0.790037  0.367649   \n",
       "4   0.833333        1.0   0.724138    0.677419  ...  0.790037  0.367649   \n",
       "5   0.833333        1.0   0.637931    0.677419  ...  0.790037  0.367649   \n",
       "\n",
       "   var28(t)  var29(t)  var30(t)  var31(t)  var32(t)  var33(t)  var34(t)  speed  \n",
       "1       0.0       1.0       1.0      0.25       0.0  0.866667       0.6  47.22  \n",
       "2       0.0       1.0       1.0      0.25       0.0  0.866667       0.6  43.49  \n",
       "3       0.0       1.0       1.0      0.25       0.0  0.866667       0.6  37.90  \n",
       "4       0.0       1.0       1.0      0.25       0.0  0.866667       0.6  37.28  \n",
       "5       0.0       1.0       1.0      0.25       0.0  0.866667       0.6  39.76  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250190, 61)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187642, 1, 68) (187642,) (62548, 1, 68) (62548,)\n"
     ]
    }
   ],
   "source": [
    "# to split a training and test sample\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "train, test = train_test_split(reframed, random_state=0)\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train.drop(['speed'],1), train['speed']\n",
    "test_X, test_y = test.drop(['speed'],1), test['speed']\n",
    "\n",
    "# to values\n",
    "train_X, train_y = train_X.values, train_y.values\n",
    "test_X, test_y = test_X.values, test_y.values\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=25, batch_size=72, validation_data=(test_X, test_y), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_test = traffic_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 34)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour',\n",
       "       'minute', 'poly_num', 'speed_id', 'speed_limit', 'lion_id', 'SeqNum',\n",
       "       'StreetCode', 'StreetWidt', 'BikeLane', 'Number_Tra', 'Number_Tot',\n",
       "       'weekend', 'morn_rush_hr', 'eve_rush_hr', 'morning_rush_avg_spd',\n",
       "       'evening_rush_avg_spd', 'wknd_avg_spd', 'overall_avg_spd',\n",
       "       'overall_std_speed', 'speed_index', 'Snow_Prior_codes', 'NonPed_codes',\n",
       "       'RB_Layer_codes', 'SegmentTyp_codes', 'FeatureTyp_codes',\n",
       "       'Street_codes', 'BOROUGH_codes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 74250000 into shape (2250000,2,33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-18985e49d4e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# reshape input to be 3D [samples, timesteps, features]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 74250000 into shape (2250000,2,33)"
     ]
    }
   ],
   "source": [
    "# to split a training and test sample\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "train, test = train_test_split(traffic_test, random_state=0)\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train.drop(['speed_index'],1), train['speed_index']\n",
    "test_X, test_y = test.drop(['speed_index'],1), test['speed_index']\n",
    "# to values\n",
    "train_X, train_y = train_X.values, train_y.values\n",
    "test_X, test_y = test_X.values, test_y.values\n",
    "3\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250000 samples, validate on 750000 samples\n",
      "Epoch 1/50\n",
      "2250000/2250000 [==============================] - 66s 29us/sample - loss: 25.7389 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 2/50\n",
      "2250000/2250000 [==============================] - 64s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 3/50\n",
      "2250000/2250000 [==============================] - 66s 29us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 4/50\n",
      "2250000/2250000 [==============================] - 62s 27us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4969 - val_accuracy: 0.1661\n",
      "Epoch 5/50\n",
      "2250000/2250000 [==============================] - 57s 25us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 6/50\n",
      "2250000/2250000 [==============================] - 57s 26us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4973 - val_accuracy: 0.1661\n",
      "Epoch 7/50\n",
      "2250000/2250000 [==============================] - 58s 26us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 8/50\n",
      "2250000/2250000 [==============================] - 58s 26us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 9/50\n",
      "2250000/2250000 [==============================] - 65s 29us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 10/50\n",
      "2250000/2250000 [==============================] - 61s 27us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 11/50\n",
      "2250000/2250000 [==============================] - 61s 27us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 12/50\n",
      "2250000/2250000 [==============================] - 68s 30us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 13/50\n",
      "2250000/2250000 [==============================] - 65s 29us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 14/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 15/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 16/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 17/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 18/50\n",
      "2250000/2250000 [==============================] - 64s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 19/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 20/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 21/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 22/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 23/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 24/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 25/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4968 - val_accuracy: 0.1661\n",
      "Epoch 26/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 27/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4969 - val_accuracy: 0.1661\n",
      "Epoch 28/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4967 - val_accuracy: 0.1661\n",
      "Epoch 29/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 30/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 31/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 32/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 33/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 34/50\n",
      "2250000/2250000 [==============================] - 62s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4968 - val_accuracy: 0.1661\n",
      "Epoch 35/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 36/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4967 - val_accuracy: 0.1661\n",
      "Epoch 37/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 38/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661ss: 25.4327 -  - ETA: 3s - loss - ETA\n",
      "Epoch 39/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 40/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4975 - val_accuracy: 0.1661\n",
      "Epoch 41/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661 loss: 25.4349 - accuracy - ETA: 1s - los\n",
      "Epoch 42/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 43/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 44/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 45/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 46/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4974 - val_accuracy: 0.1661\n",
      "Epoch 47/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4968 - val_accuracy: 0.1661\n",
      "Epoch 48/50\n",
      "2250000/2250000 [==============================] - 65s 29us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4973 - val_accuracy: 0.1661\n",
      "Epoch 49/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n",
      "Epoch 50/50\n",
      "2250000/2250000 [==============================] - 63s 28us/sample - loss: 25.4302 - accuracy: 0.1668 - val_loss: 25.4970 - val_accuracy: 0.1661\n"
     ]
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (2250000, 400)\n",
      "x_test shape: (750000, 400)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# set parameters:\n",
    "max_features = 12\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "# to split a training and test sample\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "train, test = train_test_split(traffic_test, random_state=0)\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train.drop(['speed_index'],1), train['speed_index']\n",
    "test_X, test_y = test.drop(['speed_index'],1), test['speed_index']\n",
    "# to values\n",
    "train_X, train_y = train_X.values, train_y.values\n",
    "test_X, test_y = test_X.values, test_y.values\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "train_X = sequence.pad_sequences(train_X, maxlen=maxlen)\n",
    "test_X = sequence.pad_sequences(test_X, maxlen=maxlen)\n",
    "print('x_train shape:', train_X.shape)\n",
    "print('x_test shape:', test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250000 samples, validate on 750000 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[14,367] = 436 is not in [0, 12)\n\t [[{{node embedding_2/embedding_lookup}}]] [Op:__inference_keras_scratch_graph_1481341]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-dc8997115f22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m           validation_data=(test_X, test_y))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[14,367] = 436 is not in [0, 12)\n\t [[{{node embedding_2/embedding_lookup}}]] [Op:__inference_keras_scratch_graph_1481341]"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Attempt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>LINK_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>poly_num</th>\n",
       "      <th>...</th>\n",
       "      <th>overall_avg_spd</th>\n",
       "      <th>overall_std_speed</th>\n",
       "      <th>speed_index</th>\n",
       "      <th>Snow_Prior_codes</th>\n",
       "      <th>NonPed_codes</th>\n",
       "      <th>RB_Layer_codes</th>\n",
       "      <th>SegmentTyp_codes</th>\n",
       "      <th>FeatureTyp_codes</th>\n",
       "      <th>Street_codes</th>\n",
       "      <th>BOROUGH_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91834</td>\n",
       "      <td>423</td>\n",
       "      <td>0</td>\n",
       "      <td>4616299</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>46.591828</td>\n",
       "      <td>11.256594</td>\n",
       "      <td>~ Speed Limit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5139277</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>4616364</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>39.032045</td>\n",
       "      <td>16.391624</td>\n",
       "      <td>~ Speed Limit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004934</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>4362252</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>40.963852</td>\n",
       "      <td>13.561896</td>\n",
       "      <td>~ Speed Limit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11320354</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>4456501</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>24.835551</td>\n",
       "      <td>12.576492</td>\n",
       "      <td>~ Speed Limit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1111928</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4616337</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>13.868746</td>\n",
       "      <td>5.612008</td>\n",
       "      <td>~ Speed Limit</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   ID  STATUS  LINK_ID  year  month  dayofweek  hour  minute  \\\n",
       "0       91834  423       0  4616299  2018      2          4     0      57   \n",
       "1     5139277  315       0  4616364  2018      1          4    21      13   \n",
       "2    10004934  206       0  4362252  2018     10          4    21      12   \n",
       "3    11320354  124       0  4456501  2018     11          5     8      43   \n",
       "4     1111928    1       0  4616337  2018      3          5     7      38   \n",
       "\n",
       "   poly_num  ...  overall_avg_spd  overall_std_speed    speed_index  \\\n",
       "0        44  ...        46.591828          11.256594  ~ Speed Limit   \n",
       "1        10  ...        39.032045          16.391624  ~ Speed Limit   \n",
       "2        53  ...        40.963852          13.561896  ~ Speed Limit   \n",
       "3        10  ...        24.835551          12.576492  ~ Speed Limit   \n",
       "4        13  ...        13.868746           5.612008  ~ Speed Limit   \n",
       "\n",
       "   Snow_Prior_codes  NonPed_codes  RB_Layer_codes  SegmentTyp_codes  \\\n",
       "0                 0             1               1                 1   \n",
       "1                 0             1               1                 1   \n",
       "2                 0             1               1                 1   \n",
       "3                 0             1               1                 1   \n",
       "4                 2            -1               0                 4   \n",
       "\n",
       "   FeatureTyp_codes  Street_codes  BOROUGH_codes  \n",
       "0                 0            26              3  \n",
       "1                 0            18              3  \n",
       "2                 0             9              3  \n",
       "3                 0            15              2  \n",
       "4                 0             0              2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 34)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = traffic_test.drop(['speed_index'],1)\n",
    "y = traffic_test['speed_index']\n",
    "\n",
    "# to split a training and test sample\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# to values\n",
    "x_train, x_test, y_train, y_test = x_train.values, x_test.values, y_train.values, y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250000, 1, 33) (2250000,) (2250000, 1, 33) (750000,)\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "timesteps = 1\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], timesteps, x_train.shape[1]))\n",
    "x_test = x_test.reshape((x_test.shape[0], timesteps, x_test.shape[1]))\n",
    "print(x_train.shape, y_train.shape, x_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250000 samples, validate on 750000 samples\n",
      "Epoch 1/5\n",
      "1718520/2250000 [=====================>........] - ETA: 20s - loss: 22.7776 - accuracy: 0.0198"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-36fc18037e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m model.fit(x_train, y_train,\n\u001b[1;32m     21\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 33\n",
    "num_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(64, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(64))  # return a single vector of dimension 32\n",
    "model.add(Dense(50, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_hinge',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=120, epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
