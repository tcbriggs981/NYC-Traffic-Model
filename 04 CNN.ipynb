{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will build off of the supervised learning techniques and data preparation exhibibited in the previous notebooks by applying the data to both a Convolutional Neural Network and a Long Short Term Memory Network (a type of recurring neural network)\n",
    "\n",
    "For both of these deep learning models, I will be applying the classification problem. So the outcome variable will be the Speed Index that was used to predict in the previous notebook. \n",
    "\n",
    "As you'll see, to get a convolutional neural network to work on the two dimensional data I fed into the supervised and unsupervised learning problems, a great deal of data transformation will need to happen. First of which is adding a third dimension to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /usr/local/lib/python3.7/site-packages (2.0.0a0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.12.0)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.16.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.1.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (0.32.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.19.0)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.6.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import scipy\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For the network\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import ipywidgets as iw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "!pip install tensorflow==2.0.0-alpha0\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers, models, optimizers, metrics \n",
    "\n",
    "\n",
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Import various componenets for model building\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import LSTM, Input, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from tensorflow.python.keras import backend as k\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Going to take a little bit, as the file is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_18_m = pd.read_csv('traffic_18_m.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Outcome Variable\n",
    "This is pulling the same code that was used to recreate the variable during the supervised learning phase. Trying to learn on the variable as it previously was was too hard. So this shortens the amount of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_18_m['speed_index'] = 0\n",
    "\n",
    "# Minus\n",
    "traffic_18_m['speed_index'].loc[(traffic_18_m['SPEED']<=(traffic_18_m['speed_limit']-10))&\n",
    "                                  (traffic_18_m['SPEED']>(traffic_18_m['speed_limit']-20))] = -10\n",
    "traffic_18_m['speed_index'].loc[(traffic_18_m['SPEED']<=(traffic_18_m['speed_limit']-20))] = -20\n",
    "\n",
    "\n",
    "# Minus\n",
    "traffic_18_m['speed_index'].loc[(traffic_18_m['SPEED']>=(traffic_18_m['speed_limit']+10))&\n",
    "                                  (traffic_18_m['SPEED']<(traffic_18_m['speed_limit']+20))] = 10\n",
    "traffic_18_m['speed_index'].loc[(traffic_18_m['SPEED']>=(traffic_18_m['speed_limit']+20))] = 20\n",
    "\n",
    "traffic_18_m['speed_index'].loc[traffic_18_m['SPEED']==0] = -100\n",
    "\n",
    "\n",
    "\n",
    "#traffic_18_m['speed_index'] = traffic_18_m['speed_index'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the data\n",
    "500,000 of each variable for a total of 3,000,000 rows. Will probably need to shorten this even farther later but for now it will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10     500000\n",
       "-20     500000\n",
       "-100    500000\n",
       " 20     500000\n",
       " 10     500000\n",
       " 0      500000\n",
       "Name: speed_index, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_s = traffic_18_m.sample(frac=1, random_state=1)\n",
    "\n",
    "traffic_cnn = pd.DataFrame(columns=traffic_s.columns)\n",
    "\n",
    "for i in traffic_s.speed_index.unique():\n",
    "    label = traffic_s.loc[traffic_s['speed_index']==i][:500000]\n",
    "    traffic_cnn = pd.concat([traffic_cnn, label])\n",
    "    \n",
    "traffic_cnn.speed_index.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "traffic_cnn = traffic_cnn.drop(columns=['Unnamed: 0','index','LINK_POINTS',\n",
    "                                        'ENCODED_POLY_LINE','ENCODED_POLY_LINE_LVLS',\n",
    "                                        'TRANSCOM_ID','Join_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numeric and to codes\n",
    "\n",
    "tonumeric = ['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour','minute','poly_num','BikeLane', \n",
    "             'weekend','morn_rush_hr', 'eve_rush_hr','Number_Tot','Number_Tra','SeqNum','StreetCode','lion_id',\n",
    "             'speed_id','speed_limit']\n",
    "tocategory = ['Snow_Prior','NonPed','RB_Layer','SegmentTyp','FeatureTyp','Street','BOROUGH']\n",
    "\n",
    "# turn columns into numberic\n",
    "for i in tonumeric:\n",
    "    traffic_cnn[i] = pd.to_numeric(traffic_cnn[i])\n",
    "\n",
    "# to a category then immediately into a coded column\n",
    "for i in tocategory:\n",
    "    traffic_cnn[i] = traffic_cnn[i].astype('category')\n",
    "    traffic_cnn[i+'_codes'] = traffic_cnn[i].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of null values\n",
    "traffic_cnn = traffic_cnn.drop(columns='NonPed')\n",
    "traffic_cnn = traffic_cnn.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric columns\n",
    "traffic_cnn = traffic_cnn.drop(['DATA_AS_OF','OWNER', 'BOROUGH','LINK_NAME','RecordedAtTime',\n",
    "                                'LINK_START','LINK_END','LINK_MIDDLE','Street', 'FeatureTyp', 'SegmentTyp', \n",
    "                                'RB_Layer', 'TrafDir','Snow_Prior', 'TRAVEL_TIME','SPEED'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                        int64\n",
       "STATUS                    int64\n",
       "LINK_ID                   int64\n",
       "year                      int64\n",
       "month                     int64\n",
       "dayofweek                 int64\n",
       "hour                      int64\n",
       "minute                    int64\n",
       "poly_num                  int64\n",
       "speed_id                  int64\n",
       "speed_limit               int64\n",
       "lion_id                   int64\n",
       "SeqNum                    int64\n",
       "StreetCode                int64\n",
       "StreetWidt              float64\n",
       "BikeLane                  int64\n",
       "Number_Tra                int64\n",
       "Number_Tot                int64\n",
       "weekend                   int64\n",
       "morn_rush_hr              int64\n",
       "eve_rush_hr               int64\n",
       "morning_rush_avg_spd    float64\n",
       "evening_rush_avg_spd    float64\n",
       "wknd_avg_spd            float64\n",
       "overall_avg_spd         float64\n",
       "overall_std_speed       float64\n",
       "speed_index              object\n",
       "Snow_Prior_codes           int8\n",
       "NonPed_codes               int8\n",
       "RB_Layer_codes             int8\n",
       "SegmentTyp_codes           int8\n",
       "FeatureTyp_codes           int8\n",
       "Street_codes               int8\n",
       "BOROUGH_codes              int8\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if everything is all right\n",
    "traffic_cnn.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin actually setting up the data to be run through the neural network. First off, I will take a sample out of the data that will allow me to run the network much faster. I had tried to run it on the full 3 million but it was taking a ridiculous amount of time and I noticed that removing data didn't affect the accuracy or loss of the network a great deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (30000, 1, 33)\n",
      "x_test shape: (30000, 1, 33)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# pull out a sample\n",
    "traffic_test_sm = traffic_cnn.sample(60000, random_state=1)\n",
    "\n",
    "\n",
    "# to split a training and test sample\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "train, test = train_test_split(traffic_test_sm, random_state=0, train_size=.5)\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train.drop(['speed_index'],1), train['speed_index']\n",
    "test_X, test_y = test.drop(['speed_index'],1), test['speed_index']\n",
    "\n",
    "# to values\n",
    "train_X, train_y = train_X.values, train_y.values\n",
    "test_X, test_y = test_X.values, test_y.values\n",
    "\n",
    "# Reshape into 3d\n",
    "train_X = np.array(train_X).reshape(train_X.shape[0], -1, train_X.shape[1]) \n",
    "test_X = np.array(train_X).reshape(test_X.shape[0], -1, test_X.shape[1]) \n",
    "\n",
    "# What's the output?\n",
    "print('x_train shape:', train_X.shape)\n",
    "print('x_test shape:', test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the outcome to categorical\n",
    "The convolutional neural network only takes a binary outcome, so we have to update the variable so each row has an array of binary outcomes. Luckily there is a keras utility that can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_y = tensorflow.keras.utils.to_categorical(train_y, num_classes=100, dtype='object')\n",
    "test_y = tensorflow.keras.utils.to_categorical(test_y, num_classes=100, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Network\n",
    "For the network, I am two rows of two networks with a pooling layer between them. Then finally a dropout layer to flatten then a dense layer that runs the outcome through a traditional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 13s 211us/sample - loss: 44.9561 - accuracy: 0.9883\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0245 - accuracy: 0.9901\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0244 - accuracy: 0.9901- loss: 0.024\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901- loss: 0.0244 \n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901-\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901- loss: 0.0244 - accura\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901- loss: 0.0244 - accura\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901- loss: 0.0244 - accura\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0244 - accuracy: 0.9901- loss: 0.0244 - \n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901- loss: 0.0244 - ac\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901- loss: 0.024\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901- loss: 0.0244 - accuracy: 0.\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 12s 195us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0244 - accuracy: 0.9901\n",
      "20000/20000 [==============================] - 2s 93us/sample - loss: 0.0244 - accuracy: 0.9900s - loss: 0.0244  - ETA: 0s - loss: 0.0244 - accuracy - ETA: 0s - loss: 0.0244 - accuracy:  - ETA: 0s - loss: 0.0244 \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 1, activation='relu', input_shape=(1, train_X.shape[2])))\n",
    "model.add(Conv1D(64, 1, activation='relu'))\n",
    "model.add(MaxPooling1D(1))\n",
    "model.add(Conv1D(128, 1, activation='relu'))\n",
    "model.add(Conv1D(128, 1, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_X, train_y, batch_size=8, epochs=50)\n",
    "score = model.evaluate(test_X, test_y, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "Here I set up the LSTM in the same way that I set up the CNN. The data needs to be in 3 dimensions but the 2nd dimension is about how many steps back the model takes as it is running. I pull a little bit larger of a sample here to see if it affects much as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_test = traffic_cnn.sample(80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 34)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'STATUS', 'LINK_ID', 'year', 'month', 'dayofweek', 'hour',\n",
       "       'minute', 'poly_num', 'speed_id', 'speed_limit', 'lion_id', 'SeqNum',\n",
       "       'StreetCode', 'StreetWidt', 'BikeLane', 'Number_Tra', 'Number_Tot',\n",
       "       'weekend', 'morn_rush_hr', 'eve_rush_hr', 'morning_rush_avg_spd',\n",
       "       'evening_rush_avg_spd', 'wknd_avg_spd', 'overall_avg_spd',\n",
       "       'overall_std_speed', 'speed_index', 'Snow_Prior_codes', 'NonPed_codes',\n",
       "       'RB_Layer_codes', 'SegmentTyp_codes', 'FeatureTyp_codes',\n",
       "       'Street_codes', 'BOROUGH_codes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 33) (60000, 100) (20000, 1, 33) (20000, 100)\n"
     ]
    }
   ],
   "source": [
    "# to split a training and test sample\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "train, test = train_test_split(traffic_test, random_state=0)\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train.drop(['speed_index'],1), train['speed_index']\n",
    "test_X, test_y = test.drop(['speed_index'],1), test['speed_index']\n",
    "# to values\n",
    "train_X, train_y = train_X.values, train_y.values\n",
    "test_X, test_y = test_X.values, test_y.values\n",
    "\n",
    "train_y = tensorflow.keras.utils.to_categorical(train_y, num_classes=100, dtype='object')\n",
    "test_y = tensorflow.keras.utils.to_categorical(test_y, num_classes=100, dtype='object')\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Network\n",
    "Here I set up a fairly simple LSTM. With just one layer, then a dense output. This was more of an exercise to see how the data performed on the CNN but I wanted to take it a bit further and explore the LSTM. A possible next step here would be to stack some LSTM's on top of each other but even this simple version performs so well that it might not be worth the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 20000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.1080 - accuracy: 0.9883 - val_loss: 0.1029 - val_accuracy: 0.9900\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0977 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900loss: 0.0 - ETA: 0s - los\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0900 - accuracy: 0.9900 - val_loss: 0.0901 - val_accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(100))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just an exercise to see how these two types of neural networks would work with the traffic data. I was a little nervous running non-image data through the CNN but it worked far better than I thought. I think idea of the data being so constrained to time helped play into the outcome, especially for the LSTM.\n",
    "\n",
    "If given more time, I would set the data up as more of a traditional time-series problem, and then set up a Recurring Neural Network and LSTM to then pass the data through. In it's current state, the data definitely has a time factor to it but it isn't as straightforward as it would be in a time-series scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
